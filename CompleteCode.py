import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st
import warnings
import zipfile
import shutil
from datetime import datetime
from pathlib import Path
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score
from concrete.ml.sklearn import XGBRegressor, XGBClassifier
from concrete.ml.deployment import FHEModelClient
from dotenv import load_dotenv
import google.generativeai as genai
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
import time
from concrete.ml.common.serialization.dumpers import dumps

def run_rppg_module():
    import cv2
    import numpy as np
    from scipy.signal import butter, filtfilt
    from scipy.fft import fft, fftfreq
    import tenseal as ts
    from threading import Thread
    import queue

    def butter_bandpass(lowcut, highcut, fs, order=5):
        nyq = 0.5 * fs
        low = lowcut / nyq
        high = highcut / nyq
        b, a = butter(order, [low, high], btype='band')
        return b, a

    def bandpass_filter(data, lowcut, highcut, fs, order=5):
        b, a = butter_bandpass(lowcut, highcut, fs, order=order)
        y = filtfilt(b, a, data)
        return y

    def capture_frames(duration, frame_queue, status_queue):
        """Capture frames in a separate thread"""
        cam = cv2.VideoCapture(0)
        if not cam.isOpened():
            status_queue.put("error")
            return

        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        
        means_face = []
        means_forehead = []
        start_time = time.time()
        frame_count = 0
        
        status_queue.put("recording")
        
        while True:
            ret, frame = cam.read()
            if not ret:
                break
                
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(gray, 1.3, 5)
            
            if len(faces) > 0:
                (x, y, w, h) = faces[0]
                
                # Extract face region
                face_region = frame[y:y+h, x:x+w]
                green_face = face_region[:, :, 1]
                mean_face = np.mean(green_face)
                means_face.append(mean_face)
                
                # Extract forehead region
                forehead = frame[y:y + h // 5, x:x + w]
                green_forehead = forehead[:, :, 1]
                mean_forehead = np.mean(green_forehead)
                means_forehead.append(mean_forehead)
                
                # Draw rectangles
                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)
                cv2.rectangle(frame, (x, y), (x + w, y + h // 5), (0, 255, 0), 2)
            
            # Send frame to main thread
            if not frame_queue.full():
                frame_queue.put((frame, len(means_face), time.time() - start_time))
            
            frame_count += 1
            elapsed = time.time() - start_time
            
            if elapsed >= duration:
                break
                
            time.sleep(0.03)
        
        cam.release()
        
        # Send final data
        fps = frame_count / duration if duration > 0 else 30
        frame_queue.put(("DONE", means_face, means_forehead, fps))
        status_queue.put("done")

    st.title("üíì Live rPPG Heart Rate Estimation")

    # Initialize session state
    if "recording" not in st.session_state:
        st.session_state["recording"] = False
    if "capture_thread" not in st.session_state:
        st.session_state["capture_thread"] = None
    if "frame_queue" not in st.session_state:
        st.session_state["frame_queue"] = None
    if "status_queue" not in st.session_state:
        st.session_state["status_queue"] = None

    duration = st.slider("Recording duration (seconds)", 30, 60, 30)

    if st.button("Start Recording") and not st.session_state["recording"]:
        st.session_state["recording"] = True
        st.session_state["frame_queue"] = queue.Queue(maxsize=10)
        st.session_state["status_queue"] = queue.Queue()
        
        # Start capture thread
        thread = Thread(target=capture_frames, args=(duration, st.session_state["frame_queue"], st.session_state["status_queue"]))
        thread.daemon = True
        thread.start()
        st.session_state["capture_thread"] = thread

    if st.session_state["recording"]:
        st.info("üìπ Recording in progress...")
        
        # Create placeholders
        progress_bar = st.progress(0)
        image_placeholder = st.empty()
        status_placeholder = st.empty()
        
        # Check for status updates
        try:
            while not st.session_state["status_queue"].empty():
                status = st.session_state["status_queue"].get_nowait()
                if status == "error":
                    st.error("‚ùå Could not open webcam. Please ensure it is connected and not in use.")
                    st.session_state["recording"] = False
                    return
                elif status == "done":
                    st.session_state["recording"] = False
        except queue.Empty:
            pass
        
        # Process frames
        try:
            while not st.session_state["frame_queue"].empty():
                frame_data = st.session_state["frame_queue"].get_nowait()
                
                if isinstance(frame_data[0], str) and frame_data[0] == "DONE":

                    # Process final results
                    _, means_face, means_forehead, fps = frame_data
                    
                    if len(means_face) > 0 and len(means_forehead) > 0:
                        st.write(f"‚úÖ Estimated FPS: {fps:.2f}")
                        
                        # Process signals
                        signal_face = np.array(means_face)
                        signal_forehead = np.array(means_forehead)
                        combined_signal = (signal_face + signal_forehead) / 2
                        signal_centered = combined_signal - np.mean(combined_signal)
                        
                        # Encryption demo
                        try:
                            context = ts.context(
                                ts.SCHEME_TYPE.CKKS,
                                poly_modulus_degree=8192,
                                coeff_mod_bit_sizes=[40, 20, 40]
                            )
                            context.generate_galois_keys()
                            context.generate_relin_keys()
                            context.global_scale = 2**40 
                            
                            enc = ts.ckks_vector(context, signal_centered.tolist())
                            dec_result = (enc + 1).decrypt()
                            st.write(f"üîê Encrypted-Decrypted preview: {dec_result[:5]}")
                        except Exception as e:
                            st.warning(f"‚ö†Ô∏è Encryption demo failed: {e}")
                        
                        # Filter and analyze
                        if len(signal_centered) > 10:  # Ensure we have enough data
                            filtered = bandpass_filter(signal_centered, 0.7, 4, fs=fps, order=3)
                            
                            # FFT analysis
                            n = len(filtered)
                            freqs = fftfreq(n, d=1/fps)
                            fft_values = np.abs(fft(filtered))**2
                            
                            # Find peak in heart rate range
                            idx = np.where((freqs >= 0.7) & (freqs <= 4))
                            if len(idx[0]) > 0:
                                freqs_filtered = freqs[idx]
                                fft_values_filtered = fft_values[idx]
                                peak_freq = freqs_filtered[np.argmax(fft_values_filtered)]
                                bpm = peak_freq * 60
                                st.success(f"‚ù§Ô∏è Estimated BPM: {bpm:.2f}")
                            else:
                                st.warning("‚ö†Ô∏è No clear heart rate signal detected")
                            
                            # Create plots
                            fig, axs = plt.subplots(2, 1, figsize=(12, 6))
                            
                            # Signal plot
                            axs[0].plot(signal_centered, label="Raw combined", alpha=0.7)
                            axs[0].plot(filtered, label="Filtered", linewidth=2)
                            axs[0].legend()
                            axs[0].set_title("Raw & Filtered Signal")
                            axs[0].set_xlabel("Frame")
                            axs[0].set_ylabel("Intensity")
                            
                            # FFT plot
                            pos_freqs = freqs[freqs > 0]
                            pos_fft = fft_values[freqs > 0]
                            axs[1].plot(pos_freqs * 60, pos_fft)
                            axs[1].set_title("FFT Spectrum")
                            axs[1].set_xlabel("Frequency (BPM)")
                            axs[1].set_ylabel("Power")
                            axs[1].set_xlim(40, 240)  # Typical heart rate range
                            
                            plt.tight_layout()
                            st.pyplot(fig)
                            plt.close()
                        else:
                            st.warning("‚ö†Ô∏è Insufficient data collected")
                    else:
                        st.warning("‚ö†Ô∏è No face detected during recording")
                    
                    st.session_state["recording"] = False
                    break
                else:
                    # Display current frame
                    frame, sample_count, elapsed_time = frame_data
                    image_placeholder.image(frame, channels="BGR", width=400)
                    progress_bar.progress(min(elapsed_time / duration, 1.0))
                    status_placeholder.text(f"Samples collected: {sample_count}")
                    
        except queue.Empty:
            pass
        
        # Auto-refresh while recording
        if st.session_state["recording"]:
            time.sleep(0.1)
            st.rerun()

    # Stop button
    if st.session_state["recording"]:
        if st.button("Stop Recording"):
            st.session_state["recording"] = False
            st.info("Recording stopped")


warnings.filterwarnings("ignore")
# Configure page
st.set_page_config(
    page_title="Health & Fitness AI Dashboard", 
    layout="wide", 
    initial_sidebar_state="collapsed",
    page_icon="üí™"
)
# Initialize session state flags
if "run_rppg_now" not in st.session_state:
    st.session_state["run_rppg_now"] = False

# Custom CSS for beautiful styling
st.markdown("""
<style>
  @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600;700&display=swap');

  :root {
    --deloitte-green: #86BC25;
    --dark-bg: #000000;
    --light-bg: #0A0A0A;
    --white: #ffffff;
  }
.announcement-banner {
  background: linear-gradient(135deg, #1f4037 0%, #86BC25 100%);
  padding: 1.5rem;
  border-radius: 15px;
  color: white;
  box-shadow: 0 5px 15px rgba(0,0,0,0.3);
  font-size: 1.1rem;
  font-weight: 600;
  text-align: right;
  margin-top: -2rem;
  margin-bottom: 1rem;
}
.scrollable-logs {
    max-height: 500px;
    overflow-y: scroll;
    padding: 1rem;
    background-color: #1c1c1c;
    border: 2px solid #333;
    border-radius: 15px;
    font-size: 0.95rem;
    line-height: 1.6;
}

  body {
    background: var(--dark-bg);
    color: var(--white);
    font-family: 'Poppins', sans-serif;
  }

  .main-header {
    background: linear-gradient(135deg, var(--dark-bg) 0%, var(--deloitte-green) 100%);
    padding: 2rem;
    border-radius: 15px;
    text-align: center;
    margin-bottom: 2rem;
    box-shadow: 0 10px 30px rgba(0,0,0,0.3);
  }
    .feature-list {
  background: var(--light-bg);
  color: var(--white);
  padding: 2rem;
  border-radius: 15px;
  margin: 3rem 0;
  box-shadow: 0 5px 20px rgba(0,0,0,0.2);
}

.feature-list h3 {
  color: var(--deloitte-green);
  margin-bottom: 1rem;
}

.feature-list ul {
  font-size: 1.1rem;
  line-height: 1.8;
}

.feature-list li {
  margin: 0.5rem 0;
}

  .main-header h1 {
    font-size: 3rem;
    font-weight: 700;
    margin-bottom: 0.5rem;
    color: var(--deloitte-green);
    text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
  }

  .main-header p {
    font-size: 1.2rem;
    font-weight: 300;
    opacity: 0.9;
    color: var(--white);
  }

  .upload-section {
    background: var(--light-bg);
    border: 3px dashed var(--deloitte-green);
    padding: 2rem;
    border-radius: 15px;
    text-align: center;
    margin: 2rem 0;
    box-shadow: 0 10px 30px rgba(0,0,0,0.4);
    color: var(--white);
  }

  .feature-card {
    background: var(--light-bg);
    padding: 1.5rem;
    border-radius: 10px;
    box-shadow: 0 5px 15px rgba(0,0,0,0.2);
    margin: 1rem 0;
    border-left: 4px solid var(--deloitte-green);
    color: var(--white);
  }

  .feature-card h3 {
    color: var(--deloitte-green);
    margin-bottom: 0.5rem;
  }

  .processing-animation {
    text-align: center;
    padding: 2rem;
    background: linear-gradient(135deg, #223322 0%, var(--deloitte-green) 100%);
    border-radius: 15px;
    margin: 2rem 0;
    color: var(--white);
  }

  .success-message {
    background: linear-gradient(135deg, #4CAF50 0%, var(--deloitte-green) 100%);
    padding: 1.5rem;
    border-radius: 10px;
    text-align: center;
    margin: 1rem 0;
    color: var(--white);
    font-weight: 600;
  }

  .stButton > button {
    background: var(--deloitte-green);
    color: var(--dark-bg);
    border: none;
    padding: 0.75rem 2rem;
    border-radius: 25px;
    font-weight: 600;
    font-size: 1.1rem;
    transition: all 0.3s ease;
    box-shadow: 0 5px 15px rgba(0,0,0,0.3);
  }

  .stButton > button:hover {
    transform: translateY(-2px);
    box-shadow: 0 8px 25px rgba(0,0,0,0.4);
  }

  .dashboard-section {
    background: var(--dark-bg);
    padding: 2rem;
    border-radius: 15px;
    margin: 2rem 0;
    color: var(--white);
  }

  .metric-card {
    background: var(--light-bg);
    padding: 1.5rem;
    border-radius: 10px;
    text-align: center;
    box-shadow: 0 5px 15px rgba(0,0,0,0.2);
    margin: 1rem 0;
    color: var(--white);
  }

  .chat-container {
    background: #1a1a1a;
    border-radius: 15px;
    padding: 1.5rem;
    margin: 1rem 0;
    max-height: 400px;
    overflow-y: auto;
  }

  .chat-message {
    margin: 1rem 0;
    padding: 0.75rem;
    border-radius: 10px;
    color: var(--white);
  }

  .user-message {
    background: var(--deloitte-green);
    color: var(--dark-bg);
    margin-left: 2rem;
  }

  .bot-message {
    background: #2d3748;
    color: #e2e8f0;
    margin-right: 2rem;
  }
</style>


""", unsafe_allow_html=True)

class FitbitDataProcessor:
    def __init__(self, zip_file_path, output_dir="processed_data2"):
        self.zip_file_path = zip_file_path
        self.output_dir = output_dir
        self.extracted_dir = "temp_extracted"
        
        # Create output directory if it doesn't exist
        os.makedirs(self.output_dir, exist_ok=True)
    
    def extract_zip(self):
        """Extract the ZIP file to a temporary directory"""
        print("Extracting ZIP file...")
        with zipfile.ZipFile(self.zip_file_path, 'r') as zip_ref:
            zip_ref.extractall(self.extracted_dir)
        print("ZIP file extracted successfully!")
    
    def find_file_in_extracted(self, pattern):
        """Find a file in the extracted directory that matches the pattern"""
        if not os.path.exists(self.extracted_dir):
            return None
            
        for root, dirs, files in os.walk(self.extracted_dir):
            for file in files:
                if pattern.lower() in file.lower() and file.endswith('.csv'):
                    return os.path.join(root, file)
        return None
    
    def read_csv_safe(self, filename_or_path):
        """Safely read a CSV file, return empty DataFrame if file doesn't exist"""
        # If it's a relative filename, look for it in extracted directory
        if not os.path.isabs(filename_or_path):
            file_path = self.find_file_in_extracted(filename_or_path)
        else:
            file_path = filename_or_path
            
        if file_path and os.path.exists(file_path):
            try:
                df = pd.read_csv(file_path)
                print(f"Successfully read {os.path.basename(file_path)} - Shape: {df.shape}")
                return df
            except Exception as e:
                print(f"Error reading {os.path.basename(file_path)}: {e}")
                return pd.DataFrame()
        else:
            print(f"File matching '{filename_or_path}' not found, skipping...")
            return pd.DataFrame()
    
    def standardize_date_format(self, df, date_col):
        """Standardize date format to YYYY-MM-DD"""
        if date_col in df.columns:
            # Handle various date formats
            df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.strftime('%Y-%m-%d')
        return df
    
    def create_daily_activity_merged(self):
        """Create the dailyActivity_merged.csv target file"""
        print("\n--- Creating dailyActivity_merged.csv ---")
        
        # Read all required files using pattern matching
        daily_activity = self.read_csv_safe('dailyActivity_merged')
        daily_calories = self.read_csv_safe('dailyCalories_merged')
        daily_intensities = self.read_csv_safe('dailyIntensities_merged')
        daily_steps = self.read_csv_safe('dailySteps_merged')
        sleep_day = self.read_csv_safe('sleepDay_merged')
        
        # Start with the main daily activity file or create base structure
        if not daily_activity.empty:
            result = daily_activity.copy()
            result = self.standardize_date_format(result, 'ActivityDate')
        else:
            # Create base structure from available data
            base_df = None
            for df, date_col in [(daily_calories, 'ActivityDay'), 
                               (daily_intensities, 'ActivityDay'), 
                               (daily_steps, 'ActivityDay')]:
                if not df.empty:
                    base_df = df[['Id', date_col]].copy()
                    base_df = self.standardize_date_format(base_df, date_col)
                    base_df.rename(columns={date_col: 'ActivityDate'}, inplace=True)
                    break
            
            if base_df is None:
                print("No base data found, creating dummy data...")
                # Create dummy data if no files found
                result = pd.DataFrame({
                    'Id': [1, 2, 3],
                    'ActivityDate': ['2024-01-01', '2024-01-02', '2024-01-03']
                })
            else:
                result = base_df
        
        # Merge additional data
        merge_key = ['Id', 'ActivityDate']
        
        # Merge daily calories
        if not daily_calories.empty:
            daily_calories = self.standardize_date_format(daily_calories, 'ActivityDay')
            daily_calories.rename(columns={'ActivityDay': 'ActivityDate'}, inplace=True)
            result = pd.merge(result, daily_calories, on=merge_key, how='outer', suffixes=('', '_cal'))
        
        # Merge daily intensities
        if not daily_intensities.empty:
            daily_intensities = self.standardize_date_format(daily_intensities, 'ActivityDay')
            daily_intensities.rename(columns={'ActivityDay': 'ActivityDate'}, inplace=True)
            result = pd.merge(result, daily_intensities, on=merge_key, how='outer', suffixes=('', '_int'))
        
        # Merge daily steps
        if not daily_steps.empty:
            daily_steps = self.standardize_date_format(daily_steps, 'ActivityDay')
            daily_steps.rename(columns={'ActivityDay': 'ActivityDate', 'StepTotal': 'TotalSteps'}, inplace=True)
            result = pd.merge(result, daily_steps, on=merge_key, how='outer', suffixes=('', '_steps'))
        
        # Merge sleep data
        if not sleep_day.empty:
            sleep_day = self.standardize_date_format(sleep_day, 'SleepDay')
            sleep_day.rename(columns={'SleepDay': 'ActivityDate'}, inplace=True)
            result = pd.merge(result, sleep_day, on=merge_key, how='outer', suffixes=('', '_sleep'))
        
        # Ensure all required columns exist with default values
        required_columns = [
            'Id', 'ActivityDate', 'TotalSteps', 'TotalDistance', 'TrackerDistance',
            'LoggedActivitiesDistance', 'VeryActiveDistance', 'ModeratelyActiveDistance',
            'LightActiveDistance', 'SedentaryActiveDistance', 'VeryActiveMinutes',
            'FairlyActiveMinutes', 'LightlyActiveMinutes', 'SedentaryMinutes', 'Calories',
            'TotalSleepRecords', 'TotalMinutesAsleep', 'TotalTimeInBed'
        ]
        
        for col in required_columns:
            if col not in result.columns:
                if col == 'Id':
                    result[col] = result.get('Id', range(1, len(result) + 1))
                elif col == 'ActivityDate':
                    result[col] = result.get('ActivityDate', pd.date_range('2024-01-01', periods=len(result)).strftime('%Y-%m-%d'))
                else:
                    # Generate realistic dummy data
                    if col == 'Calories':
                        result[col] = np.random.randint(1500, 3000, len(result))
                    elif col == 'TotalSteps':
                        result[col] = np.random.randint(0, 15000, len(result))
                    elif 'Minutes' in col:
                        result[col] = np.random.randint(0, 300, len(result))
                    elif 'Distance' in col:
                        result[col] = np.random.uniform(0, 10, len(result))
                    else:
                        result[col] = np.random.randint(0, 100, len(result))
        
        # Reorder columns to match target format
        result = result[required_columns]
        
        # Save to CSV
        output_path = os.path.join(self.output_dir, 'dailyActivity_merged.csv')
        result.to_csv(output_path, index=False)
        print(f"Created dailyActivity_merged.csv with {len(result)} rows")
        
        return result
    
    def create_heartrate_minutes_avg(self):
        """Create the heartrate_minutes_avg.csv target file"""
        print("\n--- Creating heartrate_minutes_avg.csv ---")
        
        heartrate_seconds = self.read_csv_safe('heartrate_seconds_merged')
        
        if not heartrate_seconds.empty and 'Time' in heartrate_seconds.columns and 'Value' in heartrate_seconds.columns:
            # Convert Time to datetime
            heartrate_seconds['Time'] = pd.to_datetime(heartrate_seconds['Time'])
            
            # Create minute-level timestamp (remove seconds)
            heartrate_seconds['ActivityMinute'] = heartrate_seconds['Time'].dt.floor('min')
            
            # Group by Id and ActivityMinute, calculate average heart rate
            result = heartrate_seconds.groupby(['Id', 'ActivityMinute'])['Value'].mean().reset_index()
            result.rename(columns={'Value': 'Avg_HeartRate'}, inplace=True)
            
            # Format ActivityMinute to match target format
            result['ActivityMinute'] = result['ActivityMinute'].dt.strftime('%Y-%m-%d %H:%M:%S')
        else:
            print("No heartrate data found, creating dummy data...")
            # Create dummy heartrate data
            dates = pd.date_range('2024-01-01', periods=1000, freq='1min')
            result = pd.DataFrame({
                'Id': np.random.choice([1, 2, 3], 1000),
                'ActivityMinute': dates.strftime('%Y-%m-%d %H:%M:%S'),
                'Avg_HeartRate': np.random.randint(60, 120, 1000)
            })
        
        # Save to CSV
        output_path = os.path.join(self.output_dir, 'heartrate_minutes_avg.csv')
        result.to_csv(output_path, index=False)
        print(f"Created heartrate_minutes_avg.csv with {len(result)} rows")
        
        return result
    
    def create_hourly_activity_merged(self):
        """Create the hourlyActivity_merged.csv target file"""
        print("\n--- Creating hourlyActivity_merged.csv ---")
        
        # Read hourly data files
        hourly_calories = self.read_csv_safe('hourlyCalories_merged')
        hourly_intensities = self.read_csv_safe('hourlyIntensities_merged')
        hourly_steps = self.read_csv_safe('hourlySteps_merged')
        
        # Start with the first available dataset
        result = None
        merge_key = ['Id', 'ActivityHour']
        
        if not hourly_calories.empty:
            result = hourly_calories.copy()
        elif not hourly_intensities.empty:
            result = hourly_intensities.copy()
        elif not hourly_steps.empty:
            result = hourly_steps.copy()
        
        if result is not None:
            # Merge other hourly datasets
            if not hourly_intensities.empty and result is not hourly_intensities:
                result = pd.merge(result, hourly_intensities, on=merge_key, how='outer', suffixes=('', '_int'))
            
            if not hourly_steps.empty and result is not hourly_steps:
                result = pd.merge(result, hourly_steps, on=merge_key, how='outer', suffixes=('', '_steps'))
            
            if not hourly_calories.empty and result is not hourly_calories:
                result = pd.merge(result, hourly_calories, on=merge_key, how='outer', suffixes=('', '_cal'))
        else:
            print("No hourly data found, creating dummy data...")
            # Create dummy hourly data
            dates = pd.date_range('2024-01-01', periods=168, freq='1h')  # 7 days of hourly data
            result = pd.DataFrame({
                'Id': np.random.choice([1, 2, 3], 168),
                'ActivityHour': dates.strftime('%Y-%m-%d %H:%M:%S'),
                'Calories': np.random.randint(50, 200, 168),
                'TotalIntensity': np.random.randint(0, 50, 168),
                'AverageIntensity': np.random.uniform(0, 2, 168),
                'StepTotal': np.random.randint(0, 1000, 168)
            })
        
        # Ensure all required columns exist
        required_columns = ['Id', 'ActivityHour', 'Calories', 'TotalIntensity', 'AverageIntensity', 'StepTotal']
        
        for col in required_columns:
            if col not in result.columns:
                if col == 'Id':
                    result[col] = np.random.choice([1, 2, 3], len(result))
                elif col == 'ActivityHour':
                    result[col] = pd.date_range('2024-01-01', periods=len(result), freq='1h').strftime('%Y-%m-%d %H:%M:%S')
                elif col == 'Calories':
                    result[col] = np.random.randint(50, 200, len(result))
                elif col == 'TotalIntensity':
                    result[col] = np.random.randint(0, 50, len(result))
                elif col == 'AverageIntensity':
                    result[col] = np.random.uniform(0, 2, len(result))
                elif col == 'StepTotal':
                    result[col] = np.random.randint(0, 1000, len(result))
        
        # Reorder columns to match target format
        result = result[required_columns]
        
        # Save to CSV
        output_path = os.path.join(self.output_dir, 'hourlyActivity_merged.csv')
        result.to_csv(output_path, index=False)
        print(f"Created hourlyActivity_merged.csv with {len(result)} rows")
        
        return result
    
    def create_weight_log_info(self):
        """Create the weightLogInfo_merged.csv as a separate file"""
        print("\n--- Creating weightLogInfo_merged.csv ---")
        
        weight_data = self.read_csv_safe('weightLogInfo_merged')
        
        if not weight_data.empty:
            # Save to CSV (already in correct format)
            output_path = os.path.join(self.output_dir, 'weightLogInfo_merged.csv')
            weight_data.to_csv(output_path, index=False)
            print(f"Created weightLogInfo_merged.csv with {len(weight_data)} rows")
        else:
            print("No weight data found, skipping...")
        
        return weight_data
    
    def cleanup(self):
        """Remove temporary extracted files"""
        print("\nCleaning up temporary files...")
        if os.path.exists(self.extracted_dir):
            shutil.rmtree(self.extracted_dir)
        print("Cleanup completed!")
    
    def process_all(self):
        """Main method to process all data"""
        print("Starting Fitbit data processing...")
        print(f"Input ZIP file: {self.zip_file_path}")
        print(f"Output directory: {self.output_dir}")
        
        try:
            # Extract ZIP file
            self.extract_zip()
            
            # Create all target files
            self.create_daily_activity_merged()
            self.create_heartrate_minutes_avg()
            self.create_hourly_activity_merged()
            self.create_weight_log_info()
            
            print("\n" + "="*50)
            print("‚úÖ All files processed successfully!")
            print(f"üìÅ Check the '{self.output_dir}' directory for results:")
            print("  - dailyActivity_merged.csv")
            print("  - heartrate_minutes_avg.csv")
            print("  - hourlyActivity_merged.csv")
            print("  - weightLogInfo_merged.csv (if available)")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error during processing: {e}")
            if 'st' in globals():
                st.error(f"‚ùå Error processing data: {e}")
            return False
            
        finally:
            # Clean up temporary files
            self.cleanup()

# Initialize session state
if 'data_processed' not in st.session_state:
    st.session_state.data_processed = False
if 'chat' not in st.session_state:
    st.session_state.chat = []
if 'processed_data_path' not in st.session_state:
    st.session_state.processed_data_path = None

# Landing Page
if not st.session_state.data_processed:
    # Main header
    st.markdown("""
    <div class="main-header">
        <h1>üèÉ‚Äç‚ôÇÔ∏è Health & Fitness AI Dashboard</h1>
        <p>Powered by Privacy-Preserving FHE Technology & Gemini AI</p>
    </div>
    """, unsafe_allow_html=True)
    st.markdown("""
    <div class="announcement-banner">
    üíì Now Measurement of Heart Rate from Facial Features is also available!
    </div>
    """, unsafe_allow_html=True)

    # Features section
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div class="feature-card">
            <h3>üîí Privacy First</h3>
            <p>Your health data is processed using Fully Homomorphic Encryption (FHE) technology, ensuring complete privacy and security.</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="feature-card">
            <h3>ü§ñ AI-Powered Insights</h3>
            <p>Get personalized health recommendations powered by Google's Gemini AI with RAG technology for accurate, context-aware responses.</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="feature-card">
            <h3>üìä Advanced Analytics</h3>
            <p>Comprehensive analysis of your fitness data with anomaly detection, predictive modeling, and interactive visualizations.</p>
        </div>
        """, unsafe_allow_html=True)
    st.markdown("""
    <div class="announcement-banner">
    üíì Now Measurement of Heart Rate from Facial Features is also available!
    </div>
    """, unsafe_allow_html=True)

    # Upload section
    st.markdown("""
    <div class="upload-section">
        <h2 style="color: white; margin-bottom: 1rem;">üìÅ Upload Your Fitbit Data</h2>
        <p style="color: rgba(255,255,255,0.9); font-size: 1.1rem;">Upload your Fitbit ZIP file to get started with personalized health insights</p>
    </div>
    """, unsafe_allow_html=True)
    # üìå 1Ô∏è‚É£ File uploader
    uploaded_file = st.file_uploader(
        "Choose your Fitbit data ZIP file",
        type=['zip'],
        help="Upload the ZIP file containing your Fitbit data export"
    )

    if uploaded_file is not None:
        st.success("‚úÖ File uploaded successfully!")

        # üìå 2Ô∏è‚É£ Process button
        if st.button("üöÄ Process Data & Launch Dashboard", type="primary"):

            st.markdown("""
            <div class="processing-animation">
                <h3>üîÑ Processing your data...</h3>
                <p>This may take a few moments. Please wait while we prepare your personalized dashboard.</p>
            </div>
            """, unsafe_allow_html=True)

            progress_bar = st.progress(0)
            status_text = st.empty()

            # üìå 3Ô∏è‚É£ Save the uploaded file to a fixed project folder
            uploads_dir = r"/home/prakharlanger/Deloitte_Capstone_Project/RAG+LLM/New_work/processed_data2"
            os.makedirs(uploads_dir, exist_ok=True)
            saved_zip_path = os.path.join(uploads_dir,uploaded_file.name)

            with open(saved_zip_path, "wb") as f:
                f.write(uploaded_file.read())

            try:
                # üìå 4Ô∏è‚É£ Process the data from saved path
                processor = FitbitDataProcessor(saved_zip_path)

                status_text.text("Extracting ZIP file...")
                progress_bar.progress(20)
                time.sleep(0.5)

                status_text.text("Processing daily activities...")
                progress_bar.progress(40)
                time.sleep(0.5)

                status_text.text("Analyzing heart rate data...")
                progress_bar.progress(60)
                time.sleep(0.5)

                status_text.text("Creating hourly activity summaries...")
                progress_bar.progress(80)
                time.sleep(0.5)

                success = processor.process_all()
                daily = pd.read_csv(f"{processor.output_dir}/dailyActivity_merged.csv")
                hr = pd.read_csv(f"{processor.output_dir}/heartrate_minutes_avg.csv")
                hourly = pd.read_csv(f"{processor.output_dir}/hourlyActivity_merged.csv")

                if success:
                    status_text.text("Finalizing dashboard...")
                    progress_bar.progress(100)
                    time.sleep(0.5)

                    st.session_state.data_processed = True
                    st.session_state.processed_data_path = processor.output_dir

                    st.markdown("""
                    <div class="success-message">
                        <h3>üéâ Data Processing Complete!</h3>
                        <p>Your personalized health dashboard is ready. Click the button below to explore your insights!</p>
                    </div>
                    """, unsafe_allow_html=True)

                    st.rerun()

                else:
                    st.error("‚ùå Failed to process data. Please check your ZIP file and try again.")

            except Exception as e:
                st.error(f"‚ùå Error processing data: {e}")

    
    # Info section
    st.markdown("""
   <div class="feature-list">
  <h3>üìã What you'll get:</h3>
  <ul>
    <li>üèÉ‚Äç‚ôÇÔ∏è <strong>Activity Analysis:</strong> Detailed breakdown of your daily activities, steps, and calories</li>
    <li>üíì <strong>Heart Rate Monitoring:</strong> Advanced heart rate analysis with anomaly detection</li>
    <li>üò¥ <strong>Sleep Insights:</strong> Sleep quality analysis and recommendations</li>
    <li>üéØ <strong>Goal Tracking:</strong> Personalized fitness goals and progress monitoring</li>
    <li>ü§ñ <strong>AI Assistant:</strong> Chat with Gemini AI for personalized health advice</li>
    <li>üìä <strong>Interactive Visualizations:</strong> Beautiful charts and graphs to understand your data</li>
  </ul>
</div>
    """, unsafe_allow_html=True)

# Dashboard Page
else:
    # Configure matplotlib for dark theme
    plt.style.use('dark_background')
    sns.set_palette("viridis")

    # Initialize session state
    if 'data_processed' not in st.session_state:
        st.session_state.data_processed = False
    if 'chat' not in st.session_state:
        st.session_state.chat = []
    if 'processed_data_path' not in st.session_state:
        st.session_state.processed_data_path = None

    # ‚úÖ One real version
    @st.cache_resource
    def load_vectorstore():
        try:
            embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
            return FAISS.load_local(
                folder_path="/home/prakharlanger/Deloitte_Capstone_Project/RAG+LLM/faiss_store",
                embeddings=embedding,
                allow_dangerous_deserialization=True
            )
        except Exception as e:
            st.error(f"Error loading vectorstore: {e}")
            return None


    def retrieve_docs(query: str, k: int = 4) -> list[str]:
        vs = load_vectorstore()
        if vs is None:
            return ["No vector store available"]
        results = vs.similarity_search(query, k=k)
        return [doc.page_content for doc in results]

    def generate_answer(query: str) -> str:
        load_dotenv()
        context = "\n\n".join(retrieve_docs(query))
        prompt = f"""
    You are a helpful assistant with expertise in:
    - Sleep, heart rate, fitness
    - Privacy-preserving AI (FHE)

    Use this context to answer:
    {context}

    User: {query}
    """
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            return "‚ö†Ô∏è GEMINI_API_KEY not found in your .env file!"
        genai.configure(api_key=api_key)

        try:
            model = genai.GenerativeModel("gemini-1.5-flash")
            response = model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            return f"‚ö†Ô∏è Gemini Error: {e}. Please check your API key."

    # Load processed data
    data_path = st.session_state.processed_data_path or "processed_data"

    @st.cache_data
    def load_processed_data():
        try:
            daily = pd.read_csv(os.path.join(data_path, 'dailyActivity_merged.csv'))
            
            hr = pd.read_csv(os.path.join(data_path, 'heartrate_minutes_avg.csv'))
            
            hourly = pd.read_csv(os.path.join(data_path, 'hourlyActivity_merged.csv'))
            
            return daily, hr, hourly
        except Exception as e:
            st.error(f"‚ùå Error loading processed data: {e}")
            return None, None, None

    # Header with reset button
    col1, col2 = st.columns([3, 1])
    with col1:
        st.title("üí™ Your Health & Fitness Dashboard")
        # rPPG Button and trigger
        if st.button("üíì Measure Live Heart Rate from Webcam"):
            st.session_state["run_rppg_now"] = True

        if st.session_state["run_rppg_now"]:
            run_rppg_module()
            st.session_state["run_rppg_now"] = True
            st.stop()

    with col2:
        if st.button("üîÑ Upload New Data", type="secondary"):
            st.session_state.data_processed = False
            st.session_state.chat = []
            st.session_state.processed_data_path = None
            st.rerun()

    # Load data
    daily, hr, hourly = load_processed_data()
    
    if daily is not None and hr is not None and hourly is not None:
        
        # Sidebar settings
        st.sidebar.header("‚öôÔ∏è Personal Settings")
        activity_goal = st.sidebar.slider(
            "Daily Active Minutes Goal",
            min_value=10,
            max_value=120,
            value=30,
            step=5,
            help="Set your daily target for very active minutes."
        )
        st.write(f"üéØ Activity goal set to: {activity_goal} minutes")
        
        # Data preprocessing
        
        hr['ActivityMinute'] = pd.to_datetime(hr['ActivityMinute'])
        hr['Date'] = hr['ActivityMinute'].dt.date
        hr = hr.dropna(subset=['Avg_HeartRate'])
        
        # Anomaly detection
        scaler = StandardScaler()
        hr['HR_Scaled'] = scaler.fit_transform(hr[['Avg_HeartRate']])
        iso = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)
        hr['IsAnomaly'] = (iso.fit_predict(hr[['HR_Scaled']]) == -1).astype(int)
        anomaly_count = hr['IsAnomaly'].sum()
        
        # Aggregate heart rate data
        hr_daily = hr.groupby(['Id', 'Date']).agg({
            'Avg_HeartRate': ['mean', 'max', 'min', 'std'],
            'IsAnomaly': 'mean'
        }).reset_index()
        hr_daily.columns = ['Id', 'ActivityDate', 'HR_Mean', 'HR_Max', 'HR_Min', 'HR_Std', 'HR_RiskScore']
        hr_daily['ActivityDate'] = pd.to_datetime(hr_daily['ActivityDate'])
        
        # Merge with daily data
        daily['ActivityDate'] = pd.to_datetime(daily['ActivityDate'])
        df = daily.merge(hr_daily, on=['Id', 'ActivityDate'], how='left')
        
        # Process hourly data for evening activity
        hourly['ActivityHour'] = pd.to_datetime(hourly['ActivityHour'])
        hourly['Hour'] = hourly['ActivityHour'].dt.hour
        hourly['Date'] = hourly['ActivityHour'].dt.date
        
        evening_df = hourly[(hourly['Hour'] >= 20) & (hourly['Hour'] <= 23)]
        evening_agg = evening_df.groupby(['Id', 'Date']).agg({
            'TotalIntensity': 'sum',
            'StepTotal': 'sum'
        }).reset_index()
        evening_agg.columns = ['Id', 'ActivityDate', 'EveningIntensity', 'EveningSteps']
        evening_agg['ActivityDate'] = pd.to_datetime(evening_agg['ActivityDate'])
        
        # Night Heart Rate
        hr['Hour'] = hr['ActivityMinute'].dt.hour
        night_hr = hr[(hr['Hour'] >= 23) | (hr['Hour'] <= 6)]
        night_agg = night_hr.groupby(['Id', 'Date']).agg({
            'Avg_HeartRate': ['mean', 'std']
        }).reset_index()
        night_agg.columns = ['Id', 'ActivityDate', 'NightHR_Mean', 'NightHR_Std']
        night_agg['ActivityDate'] = pd.to_datetime(night_agg['ActivityDate'])
        
        # Merge all data
        df = df.merge(evening_agg, on=['Id', 'ActivityDate'], how='left')
        df = df.merge(night_agg, on=['Id', 'ActivityDate'], how='left')
        
        # Fill missing values
        df[['EveningIntensity', 'EveningSteps', 'NightHR_Mean', 'NightHR_Std']] = df[
            ['EveningIntensity', 'EveningSteps', 'NightHR_Mean', 'NightHR_Std']
        ].fillna(0)
        
        # Feature engineering
        df = df.dropna(subset=['Calories', 'TotalMinutesAsleep', 'TotalTimeInBed', 'VeryActiveMinutes'])
        df['SleepQuality'] = df['TotalMinutesAsleep'] / df['TotalTimeInBed']
        df['MetActiveGoal'] = (df['VeryActiveMinutes'] >= activity_goal).astype(int)
        df['ActiveRatio'] = df['VeryActiveMinutes'] / (df['VeryActiveMinutes'] + df['SedentaryMinutes'] + 1)
        df['DistancePerStep'] = df['TotalDistance'] / (df['TotalSteps'] + 1)
        df['ActiveIntensity'] = df['TotalSteps'] / (df['FairlyActiveMinutes'] + df['VeryActiveMinutes'] + 1)
        df['UserID'] = df['Id'].astype("category").cat.codes
        
        # Data cleaning
        initial_count = len(df)
        df = df[df['Calories'] < 5000]
        df = df[df['SleepQuality'] <= 1.0]
        removed_count = initial_count - len(df)
        
        # Prepare features for ML
        features = [
            'TotalSteps', 'TotalDistance', 'TrackerDistance', 'LoggedActivitiesDistance',
            'VeryActiveDistance', 'ModeratelyActiveDistance', 'LightActiveDistance',
            'SedentaryActiveDistance', 'VeryActiveMinutes', 'FairlyActiveMinutes',
            'LightlyActiveMinutes', 'SedentaryMinutes', 'TotalSleepRecords',
            'TotalMinutesAsleep', 'TotalTimeInBed', 'ActiveRatio', 'DistancePerStep',
            'ActiveIntensity', 'HR_Mean', 'HR_Max', 'HR_Min', 'HR_Std', 'HR_RiskScore',
            'EveningIntensity', 'EveningSteps', 'NightHR_Mean', 'NightHR_Std', 'UserID'
        ]
        
        X = df[features].fillna(0).round(2)
        y_calories = df['Calories']
        y_sleep_quality = df['SleepQuality']
        y_met_goal = df['MetActiveGoal']
        
        # Train/test split
        X_train, X_test, y_cal_train, y_cal_test = train_test_split(X, y_calories, test_size=0.2, random_state=42)
        _, _, y_sleep_train, y_sleep_test = train_test_split(X, y_sleep_quality, test_size=0.2, random_state=42)
        _, _, y_goal_train, y_goal_test = train_test_split(X, y_met_goal, test_size=0.2, random_state=42)
        
        # Initialize models
        model_cal = XGBRegressor(n_estimators=100, max_depth=3)
        model_sleep = XGBRegressor(n_estimators=100, max_depth=3)
        model_goal = XGBClassifier(n_estimators=100, max_depth=3)
        
        with st.spinner("Training calories prediction model..."):
            model_cal.fit(X_train, y_cal_train)
            y_cal_pred = model_cal.predict(X_test)
            mse = mean_squared_error(y_cal_test, y_cal_pred)
            r2 = r2_score(y_cal_test, y_cal_pred)
        
        with st.spinner("Training sleep quality model..."):
            model_sleep.fit(X_train, y_sleep_train)
            y_sleep_pred = model_sleep.predict(X_test)
            mse = mean_squared_error(y_sleep_test, y_sleep_pred)
            r2 = r2_score(y_sleep_test, y_sleep_pred)
        
        # Handle classifier training carefully
        unique_classes = y_goal_train.unique()
        if len(unique_classes) < 2:
            st.warning(
                f"‚ö†Ô∏è Not enough classes in 'MetActiveGoal' to train classifier "
                f"(found only class: {unique_classes[0]}). Predictions will default to 0."
            )
            df['PredictedMetActiveGoal'] = 0
        else:
            with st.spinner("Training activity goal classifier..."):
                model_goal.fit(X_train, y_goal_train)
                y_goal_pred = model_goal.predict(X_test)
                accuracy = accuracy_score(y_goal_test, y_goal_pred)
                st.write(f"‚úÖ Activity goal classifier trained - Accuracy: {accuracy*100:.1f}%")
                df['PredictedMetActiveGoal'] = model_goal.predict(X)
        
        # Generate predictions
        df['PredictedCalories'] = model_cal.predict(X)
        df['PredictedSleepQuality'] = model_sleep.predict(X)
        
        # Show sample predictions
        st.dataframe(df[['ActivityDate', 'TotalSteps', 'PredictedCalories', 
                        'SleepQuality', 'PredictedSleepQuality', 'PredictedMetActiveGoal']].head())


        # Gemini Chat Interface
        st.subheader("üí¨ Chat with Gemini Health Assistant")
        
        # Clear Chat Button
        if st.button("üßπ Clear Chat"):
            st.session_state.chat = []
        
        # Input Form
        with st.form("chat_form", clear_on_submit=True):
            user_input = st.text_input("Ask your health question...", key="chat_input")
            submitted = st.form_submit_button("Send")
        
        # Process Input
        if submitted and user_input.strip():
            st.session_state.chat.append({"role": "user", "content": user_input})
            with st.spinner("Thinking..."):
                response = generate_answer(user_input)
            st.session_state.chat.append({"role": "bot", "content": response})
        
        # Display Chat
        if st.session_state.chat:
            chat_html = """
            <div style='max-height: 300px; overflow-y: auto; padding: 1rem; border: 1px solid #444; border-radius: 10px; background-color: #111;'>
            """
            
            for msg in st.session_state.chat:
                role = "üßë You" if msg["role"] == "user" else "ü§ñ Gemini"
                chat_html += f"<p><strong>{role}:</strong> {msg['content']}</p>"
            
            chat_html += "</div>"
            st.markdown(chat_html, unsafe_allow_html=True)
        
        # Summary Metrics
        st.subheader("üìå Summary Metrics")
        col1, col2, col3 = st.columns(3)
        col1.metric("Avg Predicted Calories", f"{df['PredictedCalories'].mean():.0f} kcal")
        col2.metric("Avg Sleep Quality", f"{df['PredictedSleepQuality'].mean():.2f}")
        col3.metric(f"% Days Met Goal (‚â•{activity_goal} mins)", f"{df['MetActiveGoal'].mean() * 100:.1f}%")
        
        # Visualizations
        row1_col1, row1_col2 = st.columns(2)
        
        with row1_col1:
            st.subheader("üéØ Activity Goal Distribution")
            fig1, ax1 = plt.subplots()

            values = df['PredictedMetActiveGoal'].value_counts().sort_index()
            index_labels = values.index.map({0: 'Not Met', 1: 'Met'})
            colors = ["black", "green"]

            ax1.pie(values, labels=index_labels, autopct='%1.1f%%', startangle=90, colors=colors[:len(values)])
            ax1.axis('equal')
            st.pyplot(fig1)

        
        with row1_col2:
            st.subheader("üß† Heart Rate Anomaly Timeline")
            fig2, ax2 = plt.subplots(figsize=(6, 4))
            hr_sample = hr.sample(n=min(1000, len(hr)), random_state=42).sort_values("ActivityMinute")
            colors = np.where(hr_sample['IsAnomaly'] == 1, 'red', 'lime')
            ax2.scatter(hr_sample['ActivityMinute'], hr_sample['Avg_HeartRate'], c=colors, alpha=0.6)
            ax2.set_title("Heart Rate Anomalies", color='white')
            ax2.set_xlabel("Time", color='white')
            ax2.set_ylabel("Avg Heart Rate", color='white')
            ax2.tick_params(colors='white')
            st.pyplot(fig2)
        
        row2_col1, row2_col2 = st.columns(2)
        
        with row2_col1:
            st.subheader("üìà Feature Correlation Heatmap")
            fig3, ax3 = plt.subplots(figsize=(8, 6))
            corr_features = ['VeryActiveMinutes', 'TotalSteps', 'Calories', 'TotalMinutesAsleep', 'HR_Mean']
            if all(col in df.columns for col in corr_features):
                corr = df[corr_features].corr()
                sns.heatmap(corr, annot=True, cmap="RdYlGn", center=0, ax=ax3)
                ax3.set_title("Feature Correlations", color='white')
            st.pyplot(fig3)
        
        with row2_col2:
            st.subheader("üò¥ Calories vs Sleep Quality")
            fig4, ax4 = plt.subplots(figsize=(6, 4))
            ax4.scatter(df['PredictedCalories'], df['PredictedSleepQuality'], c='lime', alpha=0.6)
            ax4.set_xlabel("Predicted Calories", color='white')
            ax4.set_ylabel("Sleep Quality", color='white')
            ax4.set_title("Calories vs Sleep Quality", color='white')
            ax4.tick_params(colors='white')
            st.pyplot(fig4)
        
        row3_col1, row3_col2 = st.columns(2)
        
        with row3_col1:
            st.subheader("üèÉ‚Äç‚ôÇÔ∏è Active Intensity Distribution")
            fig5, ax5 = plt.subplots(figsize=(6, 4))
            sns.histplot(df['ActiveIntensity'], kde=True, bins=30, color='lime', ax=ax5)
            ax5.set_title("Active Intensity Distribution", color='white')
            ax5.set_xlabel("Active Intensity", color='white')
            ax5.set_ylabel("Frequency", color='white')
            ax5.tick_params(colors='white')
            st.pyplot(fig5)
        
        with row3_col2:
            st.subheader("‚è∞ Activity Patterns by Hour")
            if 'Hour' in hourly.columns:
                fig6, ax6 = plt.subplots(figsize=(6, 4))
                hourly_avg = hourly.groupby('Hour')['TotalIntensity'].mean()
                ax6.plot(hourly_avg.index, hourly_avg.values, color='cyan', marker='o')
                ax6.set_title("Average Activity by Hour", color='white')
                ax6.set_xlabel("Hour of Day", color='white')
                ax6.set_ylabel("Average Intensity", color='white')
                ax6.tick_params(colors='white')
                st.pyplot(fig6)
        
        # Suggestions
        st.subheader("üí° Health Insights & Recommendations")
        
        # Calculate some insights
        avg_sleep_quality = df['PredictedSleepQuality'].mean()
        avg_calories = df['PredictedCalories'].mean()
        goal_achievement = df['MetActiveGoal'].mean() * 100
        
        insights = []
        
        if avg_sleep_quality < 0.85:
            insights.append("üò¥ **Sleep Quality Alert**: Your average sleep quality is below optimal. Consider establishing a consistent bedtime routine.")
        
        if goal_achievement < 50:
            insights.append(f"üéØ **Activity Goal**: You're meeting your {activity_goal}-minute goal only {goal_achievement:.1f}% of the time. Try breaking it into smaller chunks throughout the day.")
        
        if avg_calories > 2500:
            insights.append("üî• **High Calorie Burn**: You're burning significant calories! Make sure you're fueling your body adequately.")
        
        # Display insights
        if insights:
            for insight in insights:
                st.markdown(insight)
        else:
            st.markdown("‚úÖ **Great job!** Your health metrics are looking good. Keep up the consistent routine!")
        
        # General recommendations
        st.markdown("""
        ### General Health Tips:
        - üî¥ **Heart Rate Anomalies** may indicate:
            - Sleep deprivation, caffeine, or dehydration
            - Stress, illness, or overexertion
        - ‚úÖ **Recommended Actions**:
            - Sleep 7‚Äì9 hours daily
            - Stay hydrated throughout the day
            - Limit stimulants before bedtime
            - Allow adequate rest between intense workouts
        """)
        
        # Data Table
        st.subheader("üìã Your Health Data Overview")
        display_columns = ['ActivityDate', 'TotalSteps', 'VeryActiveMinutes', 'PredictedCalories', 'PredictedSleepQuality', 'PredictedMetActiveGoal']
        display_df = df[display_columns].copy()
        display_df['ActivityDate'] = display_df['ActivityDate'].dt.strftime('%Y-%m-%d')
        display_df['PredictedCalories'] = display_df['PredictedCalories'].round(0)
        display_df['PredictedSleepQuality'] = display_df['PredictedSleepQuality'].round(2)
        
        st.dataframe(
            display_df.sort_values('ActivityDate', ascending=False).head(20),
            use_container_width=True,
            hide_index=True
        )

    else:
        st.error("‚ùå Unable to load health data. Please check your data files and try again.")
        st.info("üìÅ Expected files: dailyActivity_merged.csv, heartrate_minutes_avg.csv, hourlyActivity_merged.csv")
